<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Recorder with Face Detection and Audio Transcription</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
</head>
<body>
    <h1>Video Recorder with Face Detection</h1>

    <!-- Video element to show the live feed -->
    <video id="videoElement" width="640" height="480" autoplay></video>
    
    <br><br>

    <!-- Buttons to control recording -->
    <button id="startButton">Start Recording</button>
    <button id="stopButton" disabled>Stop Recording</button>

    <br><br>

    <h3>Detected Text:</h3>
    <p id="transcription">Waiting for speech...</p>

    <script>
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const videoElement = document.getElementById('videoElement');
        const transcriptionElement = document.getElementById('transcription');

        let mediaRecorder;
        let videoStream;
        let audioChunks = [];
        let transcriptionText = '';
        let speechRecognition;
        let faceDetectionStarted = false;

        // Load face-api.js models from a CDN
        async function loadFaceApiModels() {
            await faceapi.nets.ssdMobilenetv1.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights');
            await faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights');
            await faceapi.nets.faceRecognitionNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights');
        }

        loadFaceApiModels().then(() => {
            console.log('Face API models loaded');
        });

        // Initialize Speech Recognition (Speech-to-Text)
        if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
            speechRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            speechRecognition.continuous = true;
            speechRecognition.interimResults = true;

            speechRecognition.onresult = (event) => {
                let transcript = '';
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    transcript += event.results[i][0].transcript;
                }
                transcriptionText = transcript;
                transcriptionElement.textContent = transcript;
            };

            speechRecognition.onerror = (event) => {
                console.error('Speech recognition error:', event);
            };
        } else {
            alert("Speech recognition is not supported in this browser.");
        }

        // Start Video Recording and Audio Transcription
        startButton.addEventListener('click', async () => {
            try {
                videoStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
                videoElement.srcObject = videoStream;

                // Initialize MediaRecorder to capture video and audio
                mediaRecorder = new MediaRecorder(videoStream);
                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };
                mediaRecorder.start();

                startButton.disabled = true;
                stopButton.disabled = false;

                // Start speech recognition
                speechRecognition.start();

                // Start face detection every 500ms
                faceDetectionStarted = true;
                detectFace();
            } catch (err) {
                console.error('Error accessing media devices:', err);
            }
        });

        // Stop Video Recording and Audio Transcription
        stopButton.addEventListener('click', () => {
            mediaRecorder.stop();
            videoStream.getTracks().forEach(track => track.stop());
            speechRecognition.stop();

            startButton.disabled = false;
            stopButton.disabled = true;

            // Output the transcribed text
            console.log('Transcription Text:', transcriptionText);
        });

        // Detect Face using face-api.js
        function detectFace() {
            if (faceDetectionStarted) {
                const canvas = faceapi.createCanvasFromMedia(videoElement);
                document.body.append(canvas);

                setInterval(async () => {
                    const detections = await faceapi.detectAllFaces(videoElement)
                        .withFaceLandmarks()
                        .withFaceDescriptors();

                    canvas?.clear();
                    faceapi.draw.drawDetections(canvas, detections);
                    faceapi.draw.drawFaceLandmarks(canvas, detections);

                    if (detections.length > 0) {
                        // If face detected, show "Face detected"
                        transcriptionElement.textContent = 'Face detected - You can start speaking!';
                    } else {
                        transcriptionElement.textContent = 'Please align your face with the camera.';
                    }
                }, 500);
            }
        }
    </script>
</body>
</html>
