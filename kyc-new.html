<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Detection and Transcription</title>
</head>
<body>
    <h1>KYC</h1>
    <video id="videoElement" width="640" height="480" autoplay muted></video>  <button id="startButton">Start Listening</button>
    <button id="stopButton" disabled>Stop Listening</button>

    <br><br>

    <h3>Transcribed Text:</h3>
    <p id="transcription">Waiting for speech...</p>

    <script>
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const videoElement = document.getElementById('videoElement');
        const transcriptionElement = document.getElementById('transcription');

        let speechRecognition;
        let allSpeechWhileRecording = "";
        let consentText = "I GIVE CONSENT TO BE POSP OF LANDMARK";
        let stream; // Store the MediaStream

        if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
            speechRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
            speechRecognition.continuous = true;
            speechRecognition.interimResults = true;

            speechRecognition.onresult = (event) => {
                let transcript = '';
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    transcript += event.results[i][0].transcript;
                }
                allSpeechWhileRecording += transcript;
                transcriptionElement.textContent = transcript;
            };

            speechRecognition.onerror = (event) => {
                console.error('Speech recognition error:', event);
            };

            speechRecognition.onstart = () => {
              transcriptionElement.textContent = 'Listening...';
            }

            speechRecognition.onend = () => {
                transcriptionElement.textContent = 'Stopped listening...';
            }
        } else {
            alert("Speech recognition is not supported in this browser.");
        }

        startButton.addEventListener('click', async () => {
            try {
                stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true }); // Get both

                videoElement.srcObject = stream;
                videoElement.muted = true; // Important: Mute the video!

                // Get the audio track from the combined stream
                const audioTracks = stream.getAudioTracks();
                const audioStream = new MediaStream(audioTracks); // Create a new stream with ONLY audio

                speechRecognition.start(); // Start speech recognition using the ONLY audio stream.

                startButton.disabled = true;
                stopButton.disabled = false;
            } catch (err) {
                console.error('Error accessing media devices:', err);
            }
        });

        stopButton.addEventListener('click', () => {
            speechRecognition.stop();
            startButton.disabled = false;
            stopButton.disabled = true;
            if (allSpeechWhileRecording.toUpperCase().includes(consentText)) {
                alert(consentText);
            } else {
                console.log(allSpeechWhileRecording);
            }

            // Stop BOTH video and audio when done!
            if (stream) {
              const tracks = stream.getTracks();
              tracks.forEach(track => track.stop());
              videoElement.srcObject = null; // Clear the video element
            }
        });
    </script>
</body>
</html>
